{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp-5KCKPAmqI",
        "outputId": "e38e29a1-8861-4eaf-dcf4-24b90b270811"
      },
      "outputs": [],
      "source": [
        "# !pip install openai"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change the following cell if you are not using a local model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import getpass, os \n",
        "secret_key = getpass.getpass('Enter OpenAI secret key: ') \n",
        "os.environ['OPENAI_API_KEY'] = secret_key\n",
        "openai.organization = 'org-TKu0EilyBURjOa59qJxK0hHb'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GzyNIXwJXmAR"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct3L_5rEXmAP",
        "outputId": "d38b1e2a-1f1c-47f8-cb22-2612ee6b030e"
      },
      "outputs": [],
      "source": [
        "# os.environ['OPENAI_API_KEY'] = 'EMPTY'\n",
        "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "# openai.base_path = \"http://localhost:1234/v1\"\n",
        "# openai.api_base = \"http://localhost:1234/v1\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OsECKdtzXmAV"
      },
      "source": [
        "## New Spanish alignment\n",
        "\n",
        "\n",
        "- [ ] Fix the output parsing part of the notebook. Perhaps make a function called `get_alignments_from_prompt_output()` instead of just splitting on highly specific string values\n",
        "- [ ] (TODO LATER) Programmatically load a Spanish translation from eBible corpus\n",
        "\n",
        "\n",
        "Some ideas for improving the initial prompt.\n",
        "\n",
        "- [x] Add language typology information about Spanish\n",
        "- [x] Add information about translation style of specific Spanish translation we are using (more literal vs. more dynamic)\n",
        "- [x] Add more examples?\n",
        "- [x] Replace current examples with examples from the language (e.g., Spanish)\n",
        "- [x] Move the rationale of each alignment before the alignment itself\n",
        "- [x] Experiment with tweaking the system prompt to see if it helps\n",
        "- [x] SIMPLIFY the prompt down to just the most relevant content (do we need a rationale? Does it work without? We can probably drop out relevant grammatical patterns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data = '/Users/ryderwishart/translators-copilot/data/bible/fraLSG.json' # french\n",
        "data = '/Users/ryderwishart/translators-copilot/data/bible/spapddpt.json' # spanish\n",
        "bible_name = 'spapddpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_broad_greek_alignment_prompt(verse):\n",
        "    bsb, macula, target = verse['bsb']['content'], verse['macula']['content'], verse['target']['content']\n",
        "    try:\n",
        "        return f'''Here are some general facts to note about Spanish:\n",
        "Spanish is a fusional language, ensure correct affix attachment; follow SVO order; mark verbs for tense, aspect, mood.\n",
        "For translating from Greek: replace Greeks's three-gender system with Spanish's two-gender system, ensuring agreement; shift to SVO order; adapt Greek Voice/Aspect/Mood markings to Spanish system.\n",
        "\n",
        "Translation style:\n",
        "The Spanish translation is  a literal translation trying to stick closely to the Greek word order, but there may occasionally be instances where Spanish phrases differ to produce a more natural translation.\n",
        "\n",
        "Here is a sentence:\n",
        "Spanish: —¿Qué es lo que ha pasado? —preguntó. Ellos respondieron: —Lo de Jesús de Nazaret. Era un profeta poderoso en obras y en palabras delante de Dios y de todo el pueblo.\n",
        "English: And He said to them What things; - And they said to Him The things concerning Jesus of Nazareth, who was a man a prophet mighty in deed and word before - God and all the people,\n",
        "Greek: καὶ εἶπεν αὐτοῖς Ποῖα;οἱ δὲ εἶπαν αὐτῷ Τὰ περὶ Ἰησοῦ τοῦ Ναζαρηνοῦ,ὃς ἐγένετο ἀνὴρ προφήτης δυνατὸς ἐν ἔργῳ καὶ λόγῳ ἐναντίον τοῦ Θεοῦ καὶ παντὸς τοῦ λαοῦ,\n",
        "\n",
        "Here is a phonological, semantic, orthographic alignment of that sentence:\n",
        "```\n",
        "[\n",
        "    {{\n",
        "        \"Spanish phrase\": \"—preguntó.\",\n",
        "        \"English phrase\": \"And He said to them\",\n",
        "        \"Greek phrase\": \"καὶ εἶπεν αὐτοῖς\"\n",
        "    }},\n",
        "    {{\n",
        "        \"Spanish phrase\": \"¿Qué es lo que ha pasado?\",\n",
        "        \"English phrase\": \"What things;\",\n",
        "        \"Greek phrase\": \"Ποῖα;\"\n",
        "    }},\n",
        "    {{\n",
        "        \"Spanish phrase\": \"Ellos respondieron\",\n",
        "        \"English phrase\": \"And they said to Him\",\n",
        "        \"Greek phrase\": \"οἱ δὲ εἶπαν αὐτοῖς\"\n",
        "    }},\n",
        "    {{\n",
        "        \"Spanish phrase\": \"—Lo de Jesús de Nazaret.\",\n",
        "        \"English phrase\": \"The things concerning Jesus of Nazareth,\",\n",
        "        \"Greek phrase\": \"Τὰ περὶ Ἰησοῦ τοῦ Ναζαρηνοῦ\"\n",
        "    }},\n",
        "    {{\n",
        "        \"Spanish phrase\": \"Era un profeta poderoso\", \n",
        "        \"English phrase\": \"who was a man a prophet mighty\", \n",
        "        \"Greek phrase\": \"ὃς ἐγένετο ἀνὴρ προφήτης δυνατὸς\"\n",
        "    }},\n",
        "        \"Spanish phrase\": \"en obras y en palabras\",\n",
        "        \"English phrase\": \"in deed and word\",\n",
        "        \"Greek phrase\": \"ἐν ἔργῳ καὶ λόγῳ\"\n",
        "    {{\n",
        "        \"Spanish phrase\": \"delante de Dios y de todo el pueblo.\",\n",
        "        \"English phrase\": \"before - God and all the people\",\n",
        "        \"Greek phrase\": \"ἐναντίον τοῦ Θεοῦ καὶ παντὸς τοῦ λαοῦ\"\n",
        "    }}\n",
        "]\n",
        "```\n",
        "\n",
        "Please also align the following sentence. Avoid including multiple phrases in a single alignment unit. You may need to break phrases  on commas or other major punctuation, including enclosing quotation marks. But you may also need to break a phrase along conjunctions or other words that typically mark the start of a new phrase:\n",
        "\n",
        "Spanish Phrase: {target}\n",
        "English Phrase: {bsb}\n",
        "Greek Phrase: {macula}\n",
        "'''\n",
        "    except Exception as e:\n",
        "        print('Error on Greek alignment prompt generation.', e)\n",
        "        return 'ERROR'\n",
        "\n",
        "def generate_broad_hebrew_alignment_prompt(verse):\n",
        "    bsb, macula, target = verse['bsb']['content'], verse['macula']['content'], verse['target']['content']\n",
        "    try:\n",
        "        return f'''Here are some general facts to note about Spanish:\n",
        "Spanish is a fusional language, ensure correct affix attachment; follow SVO order; mark verbs for tense, aspect, mood.\n",
        "For translating from Hebrew: shift to SVO order; adapt Hebrew Voice/Aspect/Mood markings to Spanish system.\n",
        "\n",
        "Translation style:\n",
        "The Spanish translation is  a literal translation trying to stick closely to the Hebrew word order, but there may occasionally be instances where Spanish phrases differ to produce a more natural translation.\n",
        "\n",
        "Here is a sentence:\n",
        "Spanish: Pero la tierra estaba desolada y vacía, y había oscuridad sobre la superficie del abismo. El Espíritu de ʼElohim se movía sobre la superficie de las aguas.\n",
        "English: Now the earth was formless and void, and darkness was over the surface of the deep. And the Spirit of God was hovering over the surface of the waters.\n",
        "Hebrew: וְהָאָ֗רֶץ הָיְתָ֥ה תֹ֨הוּ֙ וָבֹ֔הוּ וְחֹ֖שֶׁךְ עַל־פְּנֵ֣י תְה֑וֹם וְר֣וּחַ אֱלֹהִ֔ים מְרַחֶ֖פֶת עַל־פְּנֵ֥י הַמָּֽיִם׃\n",
        "\n",
        "Here is a phonological, semantic, orthographic alignment of that sentence:\n",
        "```\n",
        "[\n",
        "    {{\n",
        "        \"Spanish phrase\": \"Pero la tierra\",\n",
        "        \"English phrase\": \"Now the earth\",\n",
        "        \"Hebrew phrase\": \"וְהָאָ֗רֶץ\",\n",
        "    }},\n",
        "    {{\n",
        "        \"Spanish phrase\": \"estaba desolada\",\n",
        "        \"English phrase\": \"was formless\",\n",
        "        \"Hebrew phrase\": \"הָיְתָ֥ה תֹ֨הוּ֙\",\n",
        "    }},\n",
        "    {{\n",
        "        \"Spanish phrase\": \"y vacía,\",\n",
        "        \"English phrase\": \"and void,\",\n",
        "        \"Hebrew phrase\": \"וָבֹ֔הוּ\"\n",
        "    }},\n",
        "    {{\n",
        "        \"Spanish phrase\": \"y había oscuridad\",\n",
        "        \"English phrase\": \"and darkness\",\n",
        "        \"Hebrew phrase\": \"וְחֹ֖שֶׁךְ\",\n",
        "    }},\n",
        "    {{\n",
        "        \"Spanish phrase\": \"sobre la superficie\",\n",
        "        \"English phrase\": \"was over the surface\",\n",
        "        \"Hebrew phrase\": \"עַל־פְּנֵ֣י\",\n",
        "    }},\n",
        "    {{\n",
        "        \"Spanish phrase\": \"del abismo.\",\n",
        "        \"English phrase\": \"of the deep.\",\n",
        "        \"Hebrew phrase\": \"תְה֑וֹם\"\n",
        "    }},\n",
        "    {{\n",
        "        \"Spanish phrase\": \"El Espíritu de ʼElohim\",\n",
        "        \"English phrase\": \"And the Spirit of God\",\n",
        "        \"Hebrew phrase\": \"וְר֣וּחַ אֱלֹהִ֔ים\",\n",
        "    }},\n",
        "    {{\n",
        "        \"Spanish phrase\": \"se movía\",\n",
        "        \"English phrase\": \"was hovering\",\n",
        "        \"Hebrew phrase\": \"מְרַחֶ֖פֶת\",\n",
        "    }},\n",
        "    {{\n",
        "        \"Spanish phrase\": \"sobre la superficie de las aguas.\",\n",
        "        \"English phrase\": \"over the surface of the waters.\",\n",
        "        \"Hebrew phrase\": \"עַל־פְּנֵ֥י הַמָּֽיִם׃\"\n",
        "    }}\n",
        "]\n",
        "```\n",
        "\n",
        "Please also align the following sentence. Avoid including multiple phrases in a single alignment unit. You may need to break phrases  on commas or other major punctuation, including enclosing quotation marks. But you may also need to break a phrase along conjunctions or other words that typically mark the start of a new phrase:\n",
        "\n",
        "Spanish: {target}\n",
        "English: {bsb}\n",
        "Hebrew: {macula}\n",
        "'''\n",
        "    except Exception as e:\n",
        "        # print('Error on Hebrew alignment prompt generation.', e)\n",
        "        return 'ERROR'\n",
        "    \n",
        "book_idx = {'GEN': 1, 'EXO': 2, 'LEV': 3, 'NUM': 4, 'DEU': 5, 'JOS': 6, 'JDG': 7, 'RUT': 8, '1SA': 9, '2SA': 10,\n",
        " '1KI': 11, '2KI': 12, '1CH': 13, '2CH': 14, 'EZR': 15, 'NEH': 16, 'EST': 17, 'JOB': 18, 'PSA': 19, 'PRO': 20,\n",
        " 'ECC': 21, 'SNG': 22, 'ISA': 23, 'JER': 24, 'LAM': 25, 'EZK': 26, 'DAN': 27, 'HOS': 28, 'JOL': 29, 'AMO': 30,\n",
        " 'OBA': 31, 'JON': 32, 'MIC': 33, 'NAH': 34, 'HAB': 35, 'ZEP': 36, 'HAG': 37, 'ZEC': 38, 'MAL': 39, 'MAT': 40,\n",
        " 'MRK': 41, 'LUK': 42, 'JHN': 43, 'ACT': 44, 'ROM': 45, '1CO': 46, '2CO': 47, 'GAL': 48, 'EPH': 49, 'PHP': 50,\n",
        " 'COL': 51, '1TH': 52, '2TH': 53, '1TI': 54, '2TI': 55, 'TIT': 56, 'PHM': 57, 'HEB': 58, 'JAS': 59, '1PE': 60,\n",
        " '2PE': 61, '1JN': 62, '2JN': 63, '3JN': 64, 'JUD': 65, 'REV': 66}\n",
        "\n",
        "def generate_broad_alignment_prompt(data_element):\n",
        "    reference = data_element['vref']\n",
        "    # print(reference)\n",
        "    # print(book_idx[reference[:3]])\n",
        "    if book_idx[reference[:3]] < 40:\n",
        "        return generate_broad_hebrew_alignment_prompt(data_element)\n",
        "    else:\n",
        "        return generate_broad_greek_alignment_prompt(data_element)\n",
        "    \n",
        "with open(data, 'r') as f:\n",
        "    json_data = json.loads(f.read())\n",
        "\n",
        "# print(type(json_data))\n",
        "# for data_element in json_data[0:1]:\n",
        "#     print('>>>>', data_element)\n",
        "#     prompt = generate_broad_alignment_prompt(data_element)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBs-SZB4XmAW",
        "outputId": "eddc1ad4-3e2c-415e-ef1d-9c0e635bc438"
      },
      "outputs": [],
      "source": [
        "def get_alignments_from_prompt_output(generated_texts):\n",
        "  if generated_texts.rfind('```') != generated_texts.find('```'):\n",
        "    start_index = generated_texts.find('```')\n",
        "    end_index = generated_texts.rfind('```')\n",
        "    json_data = generated_texts[start_index + 3:end_index]\n",
        "  else:\n",
        "    start_index = generated_texts.find('```')\n",
        "    json_data = generated_texts[start_index + 3:]\n",
        "  json_data = json_data.strip()\n",
        "  # print(json_data)\n",
        "  \n",
        "  try:\n",
        "    data = json.loads(json_data)\n",
        "  except json.JSONDecodeError:\n",
        "    # print(\"\\rInvalid JSON data in the generated_texts string.\", end='')\n",
        "    return None\n",
        "  return data\n",
        "\n",
        "def align(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": f\"You are LangAlignerGPT. Analyze the user-supplied alignment examples below and follow any instructions the user gives.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    max_retries = 5\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=messages,\n",
        "                temperature=0.3,\n",
        "                n=1,\n",
        "                presence_penalty=0.5,\n",
        "                frequency_penalty=0.5,\n",
        "                api_key = secret_key\n",
        "            )\n",
        "            generated_texts = [\n",
        "                choice.message[\"content\"] for choice in response[\"choices\"]\n",
        "            ]\n",
        "            # print(generated_texts[0])\n",
        "            return generated_texts[0]\n",
        "        except (openai.error.APIConnectionError, openai.error.APIError) as e:\n",
        "            if i < max_retries - 1:  # i is zero indexed\n",
        "                continue\n",
        "            else:\n",
        "                return {\"error\": str(e)}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loop through verses (original multi-threading version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict, Union\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "\n",
        "def split_data(data: List[Dict[str, Union[str, List[Dict[str, str]]]]], num_splits: int) -> List[List[Dict[str, Union[str, List[Dict[str, str]]]]]]:\n",
        "    split_size = len(data) // num_splits\n",
        "    splits = [data[i*split_size:(i+1)*split_size] for i in range(num_splits)]\n",
        "    # If data can't be evenly split, append the remaining elements to the last split\n",
        "    if len(data) % num_splits != 0:\n",
        "        splits[-1].extend(data[num_splits*split_size:])\n",
        "    return splits\n",
        "    \n",
        "\n",
        "def process_data_element(data_element, split_number, i, total_calls_count, start_time):\n",
        "    # Check if vref already exists in the output data\n",
        "    output_file = f'/Users/ryderwishart/translators-copilot/data/alignments/{bible_name}_split_{split_number}.jsonl'\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, 'r') as f:\n",
        "            for line in f:\n",
        "                if json.loads(line)['vref'] == data_element['vref']:\n",
        "                    # print(f\"\\rSkipping vref {data_element['vref']} as it already exists in the output data.\", end=\"\")\n",
        "                    return\n",
        "\n",
        "    prompt = generate_broad_alignment_prompt(data_element)\n",
        "    \n",
        "    cleaned_result = None\n",
        "    while not cleaned_result:\n",
        "        # print('aligning...', i, 'of', len(json_data))\n",
        "        total_calls_count += 1\n",
        "        result = align(prompt)\n",
        "        cleaned_result = get_alignments_from_prompt_output(result)\n",
        "    final_result = {'vref': data_element['vref'], 'alignments': cleaned_result}\n",
        "    with open(output_file, 'a') as f:\n",
        "        f.write(json.dumps(final_result, ensure_ascii=False) + '\\n')\n",
        "        \n",
        "    output_file_line_count = 0\n",
        "    with open(output_file, 'r') as f:\n",
        "        for line in f:\n",
        "            output_file_line_count += 1\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"\\rProgress: {output_file_line_count/(len(json_data)/128)*100}%\", end=\"\")\n",
        "    print(f\"\\rRate: {output_file_line_count/elapsed_time} elements per second\", end=\"\")\n",
        "    print(f\"\\rEstimated time to completion: {((len(json_data)/128)-output_file_line_count)*(elapsed_time/output_file_line_count)/60} minutes\", end=\"\")\n",
        "\n",
        "def process_splits(num_workers, data):\n",
        "    splits = split_data(data, num_workers)\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        for i in range(num_workers):\n",
        "            total_calls_count = 0\n",
        "            start_time = time.time()\n",
        "            executor.map(process_data_element, splits[i], [i+1]*len(splits[i]), range(len(splits[i])), [total_calls_count]*len(splits[i]), [start_time]*len(splits[i]))\n",
        "\n",
        "def check_vrefs(data):\n",
        "    output_file = f'/Users/ryderwishart/translators-copilot/data/alignments/{bible_name}_split.jsonl'\n",
        "    processed_vrefs = set()\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, 'r') as f:\n",
        "            for line in f:\n",
        "                processed_vrefs.add(json.loads(line)['vref'])\n",
        "    all_vrefs = {element['vref'] for element in data}\n",
        "    return all_vrefs - processed_vrefs\n",
        "\n",
        "def process_all_vrefs(num_workers, data):\n",
        "    remaining_vrefs = check_vrefs(data)\n",
        "    while remaining_vrefs:\n",
        "        process_splits(num_workers, [element for element in data if element['vref'] in remaining_vrefs])\n",
        "        remaining_vrefs = check_vrefs(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated time to completion: -51.916918467578995 minutes"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# process_splits(128, json_data)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m process_all_vrefs(\u001b[39m128\u001b[39;49m, json_data)\n",
            "Cell \u001b[0;32mIn[11], line 68\u001b[0m, in \u001b[0;36mprocess_all_vrefs\u001b[0;34m(num_workers, data)\u001b[0m\n\u001b[1;32m     66\u001b[0m remaining_vrefs \u001b[39m=\u001b[39m check_vrefs(data)\n\u001b[1;32m     67\u001b[0m \u001b[39mwhile\u001b[39;00m remaining_vrefs:\n\u001b[0;32m---> 68\u001b[0m     process_splits(num_workers, [element \u001b[39mfor\u001b[39;49;00m element \u001b[39min\u001b[39;49;00m data \u001b[39mif\u001b[39;49;00m element[\u001b[39m'\u001b[39;49m\u001b[39mvref\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39min\u001b[39;49;00m remaining_vrefs])\n\u001b[1;32m     69\u001b[0m     remaining_vrefs \u001b[39m=\u001b[39m check_vrefs(data)\n",
            "Cell \u001b[0;32mIn[11], line 49\u001b[0m, in \u001b[0;36mprocess_splits\u001b[0;34m(num_workers, data)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_splits\u001b[39m(num_workers, data):\n\u001b[1;32m     48\u001b[0m     splits \u001b[39m=\u001b[39m split_data(data, num_workers)\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39mnum_workers) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m     50\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_workers):\n\u001b[1;32m     51\u001b[0m             total_calls_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshutdown(wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    650\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         t\u001b[39m.\u001b[39;49mjoin()\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1117\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# process_splits(128, json_data)\n",
        "process_all_vrefs(128, json_data)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merge output files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_files(num_splits):\n",
        "    with open(f'/Users/ryderwishart/translators-copilot/data/alignments/{bible_name}_merged.jsonl', 'w') as outfile:\n",
        "        for i in range(num_splits):\n",
        "            with open(f'/Users/ryderwishart/translators-copilot/data/alignments/{bible_name}_split_{i+1}.jsonl') as infile:\n",
        "                for line in infile:\n",
        "                    outfile.write(line)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Updated multi-threading attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict, Union\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import threading\n",
        "\n",
        "# Global variable to keep track of processed vrefs\n",
        "processed_vrefs = set()\n",
        "print_lock = threading.Lock()\n",
        "\n",
        "def split_data(data: List[Dict[str, Union[str, List[Dict[str, str]]]]], num_splits: int) -> List[List[Dict[str, Union[str, List[Dict[str, str]]]]]]:\n",
        "    split_size = len(data) // num_splits\n",
        "    splits = [data[i*split_size:(i+1)*split_size] for i in range(num_splits)]\n",
        "    # If data can't be evenly split, append the remaining elements to the last split\n",
        "    if len(data) % num_splits != 0:\n",
        "        splits[-1].extend(data[num_splits*split_size:])\n",
        "    return splits\n",
        "    \n",
        "def process_data_element(data_element, split_number, i, total_calls_count, start_time):\n",
        "    global processed_vrefs\n",
        "    output_file = f'/Users/ryderwishart/translators-copilot/data/alignments/{bible_name}_split_{split_number}.jsonl'\n",
        "    \n",
        "    # Check if vref already processed\n",
        "    if data_element['vref'] in processed_vrefs:\n",
        "        return\n",
        "\n",
        "    prompt = generate_broad_alignment_prompt(data_element)\n",
        "    \n",
        "    cleaned_result = None\n",
        "    while not cleaned_result:\n",
        "        total_calls_count += 1\n",
        "        result = align(prompt)\n",
        "        cleaned_result = get_alignments_from_prompt_output(result)\n",
        "        \n",
        "    final_result = {'vref': data_element['vref'], 'alignments': cleaned_result}\n",
        "    with open(output_file, 'a') as f:\n",
        "        f.write(json.dumps(final_result, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    # Update the global state of processed vrefs and print progress\n",
        "    with print_lock:\n",
        "        processed_vrefs.add(data_element['vref'])\n",
        "        elapsed_time = time.time() - start_time\n",
        "        progress_percentage = len(processed_vrefs) / len(json_data) * 100\n",
        "        rate = len(processed_vrefs) / elapsed_time\n",
        "        estimated_time_remaining = (len(json_data) - len(processed_vrefs)) * (elapsed_time / len(processed_vrefs))\n",
        "        print(f\"\\rProgress: {progress_percentage:.2f}%\", end=\"\")\n",
        "        print(f\"\\rRate: {rate:.2f} elements per second\", end=\"\")\n",
        "        print(f\"\\rEstimated time to completion: {estimated_time_remaining/60:.2f} minutes\", end=\"\")\n",
        "\n",
        "def process_splits(num_workers, data):\n",
        "    splits = split_data(data, num_workers)\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = []\n",
        "        for i in range(num_workers):\n",
        "            futures.extend(executor.submit(process_data_element, element, i+1, idx, 0, time.time()) for idx, element in enumerate(splits[i]))\n",
        "        for future in futures:\n",
        "            future.result()\n",
        "\n",
        "def check_vrefs(data):\n",
        "    return {element['vref'] for element in data} - processed_vrefs\n",
        "\n",
        "def process_all_vrefs(num_workers, data):\n",
        "    remaining_vrefs = check_vrefs(data)\n",
        "    while remaining_vrefs:\n",
        "        process_splits(num_workers, [element for element in data if element['vref'] in remaining_vrefs])\n",
        "        remaining_vrefs = check_vrefs(data)\n",
        "\n",
        "# Note: Before you run `process_all_vrefs()`, you might want to initialize the `processed_vrefs` with the vrefs already present in the files if you're continuing from a previous run.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U_G9SPqYXmAW"
      },
      "source": [
        "## Align individual chunks from output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8RlMQeTnCjj"
      },
      "outputs": [],
      "source": [
        "def get_alignments_from_prompt_output(generated_texts):\n",
        "  \n",
        "  if generated_texts.rfind('```') != generated_texts.find('```'):\n",
        "    start_index = generated_texts.find('```')\n",
        "    end_index = generated_texts.rfind('```')\n",
        "    json_data = generated_texts[start_index + 3:end_index]\n",
        "  else:\n",
        "    start_index = generated_texts.find('```')\n",
        "    json_data = generated_texts[start_index + 3:]\n",
        "  json_data = json_data.strip()\n",
        "  print(json_data)\n",
        "  \n",
        "  try:\n",
        "    data = json.loads(json_data)\n",
        "  except json.JSONDecodeError:\n",
        "    print(\"Invalid JSON data in the generated_texts string.\")\n",
        "    return None\n",
        "  return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5smIo3Yq3bD"
      },
      "outputs": [],
      "source": [
        "output = get_alignments_from_prompt_output(generated_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('C:/Users/natha/Downloads/dynamic_posalign.txt', 'w') as file:\n",
        "    file.write('[\\n')\n",
        "    for i in output:\n",
        "        file.write(json.dumps(i, ensure_ascii = False) + ',')\n",
        "    file.write(']')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def refined_greek_alignments_from_prompt_output(output, chunk):\n",
        "    return '''Here is a phrase:\n",
        "        {POS_aligned_chunks}\n",
        "\n",
        "        Please further align and break down the provided chunk into a mapping of the fewest possible tokens (sometimes multiple tokens will align to one token; that's expected) in a format similar to the following:\n",
        "\n",
        "        E.g.: \"Spanish phrase\": \"Después de esto,\",\\n\\t\"English phrase\": \"After now these things\",\\n\\t\"Greek phrase\": \"Μετὰ δὲ ταῦτα\"\n",
        "\n",
        "        - Spanish token(s)\\t\\t-->\\tEnglish token(s)\\t\\t-->\\tGreek token(s)\n",
        "        - Después de\\t\\t-->\\tAfter now\\t\\t-->\\tΜετὰ δὲ\n",
        "        - esto\\t\\t-->\\tthese things\\t\\t-->\\tταῦτα\n",
        "\n",
        "        Do not include any other information or commentary. Only tell me what the alignment is.\n",
        "\n",
        "        The chunk I want you to align is:\n",
        "            ```\n",
        "            {current_chunk}\n",
        "            ```\n",
        "        '''.format(POS_aligned_chunks=json.dumps(output, ensure_ascii = False), current_chunk=json.dumps(chunk, ensure_ascii = False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def refined_hebrew_alignments_from_prompt_output(output, chunk):\n",
        "    return '''Here is a phrase:\n",
        "        {POS_aligned_chunks}\n",
        "\n",
        "        Please further align and break down the provided chunk into a mapping of the fewest possible tokens (sometimes multiple tokens will align to one token; that's expected) in a format similar to the following:\n",
        "\n",
        "        E.g.: \"Spanish phrase\": \"El Espíritu de ʼElohim se movía sobre la superficie de las aguas.\",\\n\\t\"English phrase\": \"And the Spirit of God was hovering over the surface of the waters.\",\\n\\t\"Hebrew phrase\":  \"וְר֣וּחַ אֱלֹהִ֔ים מְרַחֶ֖פֶת עַל־פְּנֵ֥י הַמָּֽיִם׃\"\n",
        "\n",
        "        - Spanish token(s)\\t\\t-->\\tEnglish token(s)\\t\\t-->\\tHebrew token(s)\n",
        "        - El Espíritu de ʼElohim\\t\\t-->\\tAnd the Spirit of God\\t\\t-->\\tוְר֣וּחַ אֱלֹהִ֔ים\n",
        "        - se movía sobre la superficie de las aguas.\\t\\t-->\\twas hovering over the surface of the waters.\\t\\t-->\\tמְרַחֶ֖פֶת עַל־פְּנֵ֥י הַמָּֽיִם׃\n",
        "\n",
        "        Do not include any other information or commentary. Only tell me what the alignment is.\n",
        "\n",
        "        The chunk I want you to align is:\n",
        "            ```\n",
        "            {current_chunk}\n",
        "            ```\n",
        "        '''.format(POS_aligned_chunks=json.dumps(output, ensure_ascii = False), current_chunk=json.dumps(chunk, ensure_ascii = False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_texts(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": f\"You are LangAlignerGPT. Analyze the user-supplied alignment examples below and follow any instructions the user gives.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages,\n",
        "        temperature=0.3,\n",
        "        n=1,\n",
        "        presence_penalty=0.5,\n",
        "        frequency_penalty=0.5,\n",
        "    )\n",
        "\n",
        "    generated_texts_for_chunk = [\n",
        "        choice.message[\"content\"].strip() for choice in response[\"choices\"]\n",
        "\n",
        "    ]\n",
        "    \n",
        "    return generated_texts_for_chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkOBxZqlXmAX",
        "outputId": "666f2227-aace-49d1-9999-4dfb58762c6c"
      },
      "outputs": [],
      "source": [
        "final_alignments = []\n",
        "\n",
        "for chunk in output:\n",
        "  if 'Hebrew' in json.dumps(chunk, ensure_ascii = False):\n",
        "    prompt = refined_hebrew_alignments_from_prompt_output(output=output, chunk=chunk)\n",
        "  else:\n",
        "    prompt = refined_greek_alignments_from_prompt_output(output=output, chunk=chunk)\n",
        "\n",
        "  generated_texts = generate_texts(prompt)\n",
        "  \n",
        "  # Count the occurrences of 'phrase' in each element to improve odds of correct output\n",
        "  phrase_count = sum(element.count('phrase\":') for element in generated_texts)\n",
        "  \n",
        "  # Repeat until 'phrase' is found exactly three times in each element\n",
        "  while phrase_count != 3 and colon_count != 3:\n",
        "    generated_texts = generate_texts(prompt)\n",
        "    phrase_count = sum(element.count('phrase\":') for element in generated_texts)\n",
        "\n",
        "  print(generated_texts[0])\n",
        "  final_alignments.append(generated_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdveDvS9XmAX",
        "outputId": "bc5d8e83-a0bc-42ab-b038-a41e3f918f9e"
      },
      "outputs": [],
      "source": [
        "with open('C:/Users/natha/Downloads/refined_dynamic_posalign.txt', 'w') as file:\n",
        "    file.write('[\\n')\n",
        "    for i in final_alignments:\n",
        "        print(i)\n",
        "        print('-----')\n",
        "        file.write(json.dumps(i, ensure_ascii = False) + ',')\n",
        "    file.write(']')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
